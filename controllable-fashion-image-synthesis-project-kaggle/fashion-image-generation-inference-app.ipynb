{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5787457,"sourceType":"datasetVersion","datasetId":3324119},{"sourceId":14152652,"sourceType":"datasetVersion","datasetId":9020297}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 1: Clean Room Installation (with Fixed Gradio)\n\nimport os\n\n# 1. Create constraints file\nwith open(\"constraints.txt\", \"w\") as f:\n    f.write(\"numpy<2.0\\n\")\n    f.write(\"huggingface_hub<0.25.0\\n\")\n\n# 2. Uninstall potential conflicts\nprint(\"üßπ Purging conflicts...\")\nos.system(\"pip uninstall -y tensorflow tensorflow-cpu tensorflow-gpu protobuf numpy gradio gradio_client -q\")\n\n# 3. Install stack with PINNED Gradio version\nprint(\"üì¶ Installing Stack...\")\ncmd = (\n    \"pip install \"\n    \"numpy==1.26.4 \"\n    \"diffusers==0.26.3 \"\n    \"transformers==4.38.2 \"\n    \"accelerate==0.27.2 \"\n    \"peft==0.9.0 \"\n    \"ftfy \"\n    \"tensorboard \"\n    \"-c constraints.txt -q\"\n)\nos.system(cmd)\n\n# Uninstall Broken bitsandbytes\nimport os\nprint(\"üöë Removing broken bitsandbytes library...\")\n# We uninstall it so peft stops trying to import it and crashing.\nos.system(\"pip uninstall -y bitsandbytes\")\nprint(\"‚úÖ Broken library removed. Falling back to standard precision.\")\nimport numpy\nprint(f\"‚úÖ NumPy Version: {numpy.__version__}\")\n\nprint(\"‚úÖ All dependencies installed!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-14T11:38:54.868981Z","iopub.execute_input":"2025-12-14T11:38:54.869197Z","iopub.status.idle":"2025-12-14T11:40:51.363065Z","shell.execute_reply.started":"2025-12-14T11:38:54.869179Z","shell.execute_reply":"2025-12-14T11:40:51.362318Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"üßπ Purging conflicts...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Skipping tensorflow-cpu as it is not installed.\nWARNING: Skipping tensorflow-gpu as it is not installed.\n","output_type":"stream"},{"name":"stdout","text":"üì¶ Installing Stack...\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 61.0/61.0 kB 2.2 MB/s eta 0:00:00\n     ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 130.7/130.7 kB 5.4 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 18.3/18.3 MB 109.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.9/1.9 MB 80.1 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 8.5/8.5 MB 137.8 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 280.0/280.0 kB 21.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 190.9/190.9 kB 13.5 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 44.8/44.8 kB 2.6 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 417.5/417.5 kB 26.1 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 323.3/323.3 kB 20.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3.6/3.6 MB 103.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 363.4/363.4 MB 5.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 13.8/13.8 MB 115.2 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 24.6/24.6 MB 90.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 883.7/883.7 kB 43.3 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 664.8/664.8 MB 2.8 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 211.5/211.5 MB 8.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 56.3/56.3 MB 34.9 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 127.9/127.9 MB 15.0 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 207.5/207.5 MB 5.7 MB/s eta 0:00:00\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 21.1/21.1 MB 103.8 MB/s eta 0:00:00\n","output_type":"stream"},{"name":"stderr","text":"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndopamine-rl 4.1.2 requires tensorflow>=2.2.0, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, which is not installed.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.33.2 which is incompatible.\ndatasets 4.4.1 requires huggingface-hub<2.0,>=0.25.0, but you have huggingface-hub 0.24.7 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.33.2 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.2.2 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.33.2 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\n","output_type":"stream"},{"name":"stdout","text":"üöë Removing broken bitsandbytes library...\n‚úÖ Broken library removed. Falling back to standard precision.\n‚úÖ NumPy Version: 1.26.4\n‚úÖ All dependencies installed!\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Skipping bitsandbytes as it is not installed.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# CELL 2: Load Model with Fashion-Gen Dataset Support (H5 Format)\nimport torch\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport os\nimport glob\nimport h5py\nimport tempfile\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n\n# --- 1. Find LoRA Weights ---\nprint(\"üîç Searching for LoRA weights...\")\nLORA_WEIGHTS_PATH = None\n\nfor folder in os.listdir(\"/kaggle/input\"):\n    # Check for direct file in folder (your structure)\n    direct_path = f\"/kaggle/input/{folder}\"\n    weights_file = os.path. join(direct_path, \"pytorch_lora_weights.safetensors\")\n    if os.path.exists(weights_file):\n        LORA_WEIGHTS_PATH = direct_path\n        print(f\"‚úÖ Found LoRA weights at: {LORA_WEIGHTS_PATH}\")\n        break\n    \n    # Also check subfolder structure (original structure)\n    subfolder_path = f\"/kaggle/input/{folder}/fashion_lora_output\"\n    weights_file_sub = os.path. join(subfolder_path, \"pytorch_lora_weights. safetensors\")\n    if os. path.exists(weights_file_sub):\n        LORA_WEIGHTS_PATH = subfolder_path\n        print(f\"‚úÖ Found LoRA weights at: {LORA_WEIGHTS_PATH}\")\n        break\n\nif LORA_WEIGHTS_PATH is None:  \n    raise FileNotFoundError(\"‚ùå Could not find LoRA weights.  Please add your dataset as input.\")\n\n# --- 2. Extract Images from Fashion-Gen H5 Dataset ---\nprint(\"üîç Searching for Fashion-Gen H5 dataset...\")\nH5_FILE_PATH = None\nsample_image_paths = {}\n\n# Search for H5 files\nfor folder in os. listdir(\"/kaggle/input\"):\n    folder_path = f\"/kaggle/input/{folder}\"\n    h5_files = glob.glob(os.path.join(folder_path, \"*.h5\"))\n    \n    if h5_files: \n        # Prefer validation file (smaller), then train\n        for h5_file in h5_files: \n            if 'validation' in h5_file. lower():\n                H5_FILE_PATH = h5_file\n                break\n        if H5_FILE_PATH is None:\n            H5_FILE_PATH = h5_files[0]\n        \n        print(f\"‚úÖ Found Fashion-Gen H5 file:  {H5_FILE_PATH}\")\n        break\n\nif H5_FILE_PATH is None:\n    raise FileNotFoundError(\"‚ùå Could not find Fashion-Gen H5 file.\")\n\n# Extract images from H5 file\nprint(\"üì¶ Extracting images from H5 file...\")\n\n# Create a temporary directory for extracted images\nEXTRACTED_IMAGES_DIR = \"/kaggle/working/extracted_fashion_images\"\nos.makedirs(EXTRACTED_IMAGES_DIR, exist_ok=True)\n\n# Open H5 file and extract images\nwith h5py.File(H5_FILE_PATH, 'r') as h5_file:\n    print(f\"   H5 file keys: {list(h5_file.keys())}\")\n    \n    # Common key names in Fashion-Gen dataset\n    possible_image_keys = ['input_image', 'images', 'image', 'input_images', 'data']\n    \n    image_key = None\n    for key in possible_image_keys:\n        if key in h5_file:\n            image_key = key\n            break\n    \n    # If no common key found, try to find any dataset with image-like shape\n    if image_key is None: \n        for key in h5_file.keys():\n            dataset = h5_file[key]\n            if isinstance(dataset, h5py.Dataset):\n                shape = dataset.shape\n                # Check if it looks like images (N, H, W, C) or (N, C, H, W)\n                if len(shape) == 4 and (shape[-1] == 3 or shape[1] == 3):\n                    image_key = key\n                    print(f\"   Found image dataset: {key} with shape {shape}\")\n                    break\n    \n    if image_key is None: \n        print(\"   Available keys and their shapes:\")\n        for key in h5_file.keys():\n            if isinstance(h5_file[key], h5py. Dataset):\n                print(f\"   - {key}: {h5_file[key].shape}\")\n        raise KeyError(\"Could not find image data in H5 file.  Check the keys above.\")\n    \n    images_dataset = h5_file[image_key]\n    total_images = images_dataset.shape[0]\n    print(f\"   Total images in dataset: {total_images}\")\n    \n    # Extract up to 100 images (evenly spaced for variety)\n    num_to_extract = min(100, total_images)\n    indices = np.linspace(0, total_images - 1, num_to_extract, dtype=int)\n    \n    for i, idx in enumerate(indices):\n        img_array = images_dataset[idx]\n        \n        # Handle different formats (N, H, W, C) vs (N, C, H, W)\n        if img_array.shape[0] == 3:  # (C, H, W) format\n            img_array = np.transpose(img_array, (1, 2, 0))\n        \n        # Ensure uint8 format\n        if img_array. max() <= 1.0:\n            img_array = (img_array * 255).astype(np.uint8)\n        else:\n            img_array = img_array.astype(np.uint8)\n        \n        # Save image\n        img = Image.fromarray(img_array)\n        save_path = os. path.join(EXTRACTED_IMAGES_DIR, f\"fashion_{idx:05d}.png\")\n        img.save(save_path)\n        \n        sample_image_paths[f\"Image {i+1}: fashion_{idx:05d}\"] = save_path\n        \n        if (i + 1) % 20 == 0:\n            print(f\"   Extracted {i + 1}/{num_to_extract} images...\")\n\nprint(f\"‚úÖ Extracted {len(sample_image_paths)} images from Fashion-Gen dataset\")\n\n# --- 3. Load Pipeline ---\nprint(\"‚öôÔ∏è Loading ControlNet...\")\ncontrolnet = ControlNetModel. from_pretrained(\n    \"lllyasviel/control_v11p_sd15_canny\",\n    torch_dtype=torch.float16\n)\n\nprint(\"‚öôÔ∏è Loading Stable Diffusion Pipeline...\")\npipe = StableDiffusionControlNetPipeline. from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    controlnet=controlnet,\n    torch_dtype=torch.float16,\n    safety_checker=None\n).to(\"cuda\")\n\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe. scheduler.config)\n\nprint(\"‚öôÔ∏è Loading LoRA Weights...\")\npipe.load_lora_weights(LORA_WEIGHTS_PATH, weight_name=\"pytorch_lora_weights.safetensors\")\n\nprint(\"‚úÖ Model loaded successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T12:21:42.507863Z","iopub.execute_input":"2025-12-14T12:21:42.508726Z","iopub.status.idle":"2025-12-14T12:21:48.377358Z","shell.execute_reply.started":"2025-12-14T12:21:42.508698Z","shell.execute_reply":"2025-12-14T12:21:48.376793Z"}},"outputs":[{"name":"stdout","text":"üîç Searching for LoRA weights...\n‚úÖ Found LoRA weights at: /kaggle/input/fashion-lora-output\nüîç Searching for Fashion-Gen H5 dataset...\n‚úÖ Found Fashion-Gen H5 file:  /kaggle/input/fashiongen-validation/fashiongen_256_256_train.h5\nüì¶ Extracting images from H5 file...\n   H5 file keys: ['index', 'index_2', 'input_brand', 'input_category', 'input_composition', 'input_concat_description', 'input_department', 'input_description', 'input_gender', 'input_image', 'input_msrpUSD', 'input_name', 'input_pose', 'input_productID', 'input_season', 'input_subcategory']\n   Total images in dataset: 260490\n   Extracted 20/100 images...\n   Extracted 40/100 images...\n   Extracted 60/100 images...\n   Extracted 80/100 images...\n   Extracted 100/100 images...\n‚úÖ Extracted 100 images from Fashion-Gen dataset\n‚öôÔ∏è Loading ControlNet...\n‚öôÔ∏è Loading Stable Diffusion Pipeline...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6308d4da438b49ab8dbf354d4dddfbb4"}},"metadata":{}},{"name":"stderr","text":"You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n","output_type":"stream"},{"name":"stdout","text":"‚öôÔ∏è Loading LoRA Weights...\n‚úÖ Model loaded successfully!\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# CELL 3: Helper Functions\nimport random\nimport matplotlib.pyplot as plt\nfrom IPython.display import display, clear_output\n\ndef get_canny_edge(pil_image, low_threshold=100, high_threshold=200):\n    \"\"\"Extract Canny edges from an image.\"\"\"\n    img_array = np.array(pil_image)\n    if len(img_array.shape) == 2:\n        img_array = cv2.cvtColor(img_array, cv2.COLOR_GRAY2RGB)\n    edges = cv2.Canny(img_array, low_threshold, high_threshold)\n    edges_rgb = np.stack([edges] * 3, axis=-1)\n    return Image.fromarray(edges_rgb)\n\ndef generate_image(image, prompt, steps=20, guidance=7.5, seed=-1):\n    \"\"\"Generate a fashion image.\"\"\"\n    # Resize\n    image = image.resize((256, 256), Image.LANCZOS)\n    \n    # Get edges\n    edge_image = get_canny_edge(image)\n    \n    # Set seed\n    if seed == -1:\n        seed = random.randint(0, 2147483647)\n    generator = torch.Generator(device=\"cuda\").manual_seed(int(seed))\n    \n    # Generate\n    with torch.inference_mode():\n        output = pipe(\n            prompt=prompt,\n            image=edge_image,\n            num_inference_steps=int(steps),\n            guidance_scale=guidance,\n            generator=generator\n        )\n    \n    return edge_image, output. images[0], seed\n\ndef display_results(reference, edges, generated, prompt, seed):\n    \"\"\"Display results in a nice grid.\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    \n    axes[0].imshow(reference)\n    axes[0].set_title(\"Reference Image\")\n    axes[0].axis('off')\n    \n    axes[1].imshow(edges)\n    axes[1]. set_title(\"Edge Map\")\n    axes[1].axis('off')\n    \n    axes[2].imshow(generated)\n    axes[2]. set_title(\"Generated\")\n    axes[2].axis('off')\n    \n    plt.suptitle(f'Prompt: \"{prompt}\"\\nSeed:  {seed}', fontsize=10)\n    plt.tight_layout()\n    plt.show()\n\nprint(\"‚úÖ Helper functions defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T12:21:50.322858Z","iopub.execute_input":"2025-12-14T12:21:50.323458Z","iopub.status.idle":"2025-12-14T12:21:50.331954Z","shell.execute_reply.started":"2025-12-14T12:21:50.323435Z","shell.execute_reply":"2025-12-14T12:21:50.331241Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Helper functions defined.\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# CELL 4: Enhanced Interactive Widget Interface (Fixed Random Samples)\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output, HTML\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom PIL import Image\nimport random\nimport torch\n\n# --- Preset Prompts (Organized by Category) ---\nPRESET_PROMPTS = {\n    \"üß• Outerwear\": {\n        \"Black Leather Jacket\": \"A classic black leather jacket with silver zippers and fitted silhouette\",\n        \"Navy Blazer\": \"A tailored navy blue blazer with gold buttons and sharp lapels\",\n        \"Denim Jacket\": \"A vintage medium-wash denim jacket with brass buttons\",\n        \"Bomber Jacket\": \"A brown leather bomber jacket with shearling collar\",\n        \"Wool Overcoat\": \"An elegant charcoal wool overcoat with notched lapels\",\n    },\n    \"üëî Tops\": {\n        \"White Linen Shirt\": \"A crisp white linen shirt with relaxed fit\",\n        \"Silk Blouse\": \"An elegant cream silk blouse with subtle sheen\",\n        \"Cashmere Sweater\": \"A cozy beige cashmere sweater with ribbed texture\",\n        \"Striped T-Shirt\": \"A classic navy and white striped cotton t-shirt\",\n        \"Wool Cardigan\": \"A chunky cream wool cardigan with wooden buttons\",\n    },\n    \"üëó Dresses\": {\n        \"Red Silk Dress\":  \"A bright red silk evening dress with elegant draping\",\n        \"Velvet Cocktail\":  \"A deep burgundy velvet cocktail dress\",\n        \"Little Black Dress\": \"A sophisticated black fitted dress with clean lines\",\n        \"Floral Sundress\": \"A light floral print sundress with flowing skirt\",\n    },\n    \"üëñ Bottoms\":  {\n        \"Dark Jeans\": \"Classic dark indigo straight-leg denim jeans\",\n        \"Tailored Trousers\": \"Elegant black tailored wool trousers\",\n        \"Leather Pants\": \"Sleek black leather pants with slim fit\",\n    }\n}\n\n# --- Custom CSS Styling ---\nstyle_html = HTML(\"\"\"\n<style>\n.widget-label { font-weight: bold ! important; }\n. generator-title { \n    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n    color: white;\n    padding: 20px;\n    border-radius: 10px;\n    margin-bottom: 20px;\n    text-align: center;\n}\n.generator-title h1 { margin: 0; font-size: 28px; }\n. generator-title p { margin: 10px 0 0 0; opacity: 0.9; }\n. section-title { \n    color: #495057; \n    font-size: 16px; \n    font-weight: bold; \n    margin-bottom: 10px;\n    border-bottom: 2px solid #667eea;\n    padding-bottom: 5px;\n}\n. tip-box {\n    background:  linear-gradient(135deg, #f5f7fa 0%, #e4e8eb 100%);\n    border-left: 4px solid #667eea;\n    padding:  15px;\n    margin: 15px 0;\n    border-radius:  0 8px 8px 0;\n}\n.results-title {\n    background: #28a745;\n    color: white;\n    padding: 10px 20px;\n    border-radius: 8px;\n    display: inline-block;\n    margin:  20px 0 10px 0;\n}\n. status-success { color: #28a745; font-weight: bold; }\n.status-error { color: #dc3545; font-weight: bold; }\n.status-loading { color: #ffc107; font-weight: bold; }\n</style>\n\"\"\")\ndisplay(style_html)\n\n# --- Title ---\ntitle = widgets.HTML(\"\"\"\n<div class=\"generator-title\">\n    <h1>üëó Fashion Image Generator</h1>\n    <p>Transform reference images into stunning fashion designs using AI</p>\n</div>\n\"\"\")\n\n# ==================== SAMPLE IMAGE MANAGEMENT ====================\n\n# Store ALL sample paths globally\nALL_SAMPLE_PATHS = dict(sample_image_paths)  # Make a copy\nALL_SAMPLE_KEYS = list(ALL_SAMPLE_PATHS.keys())\n\nprint(f\"üìÅ Total available images: {len(ALL_SAMPLE_KEYS)}\")\n\ndef get_diverse_samples(n=10):\n    \"\"\"Select diverse samples by picking every nth image.\"\"\"\n    if len(ALL_SAMPLE_KEYS) <= n:\n        return ALL_SAMPLE_KEYS. copy()\n    step = max(1, len(ALL_SAMPLE_KEYS) // n)\n    return [ALL_SAMPLE_KEYS[i * step] for i in range(min(n, len(ALL_SAMPLE_KEYS)))]\n\ndef get_random_samples(n=10):\n    \"\"\"Get completely random samples.\"\"\"\n    return random.sample(ALL_SAMPLE_KEYS, min(n, len(ALL_SAMPLE_KEYS)))\n\n# Initialize with diverse samples\ncurrent_samples = get_diverse_samples(10)\n\n# ==================== WIDGETS ====================\n\n# --- Image Selection ---\nsample_dropdown = widgets. Dropdown(\n    options=current_samples,\n    value=current_samples[0] if current_samples else None,\n    description='',\n    layout=widgets.Layout(width='100%')\n)\n\nimage_preview = widgets.Output(layout=widgets.Layout(\n    width='100%', \n    min_height='220px',\n    display='flex',\n    justify_content='center',\n    align_items='center'\n))\n\nprev_btn = widgets. Button(description='‚óÄ Prev', layout=widgets.Layout(width='80px'))\nnext_btn = widgets.Button(description='Next ‚ñ∂', layout=widgets.Layout(width='80px'))\nrandom_img_btn = widgets. Button(description='üé≤ Random', button_style='info', layout=widgets. Layout(width='100px'))\nshuffle_btn = widgets. Button(description='üîÄ New Set', button_style='warning', layout=widgets. Layout(width='100px'))\n\n# Image counter display\nimage_counter = widgets.HTML(value=f'<small style=\"color:#6c757d\">Showing 10 of {len(ALL_SAMPLE_KEYS)} images</small>')\n\n# --- Category and Prompt Selection ---\ncategory_dropdown = widgets. Dropdown(\n    options=list(PRESET_PROMPTS.keys()),\n    value=list(PRESET_PROMPTS.keys())[0],\n    description='',\n    layout=widgets.Layout(width='100%')\n)\n\nprompt_dropdown = widgets.Dropdown(\n    options=list(PRESET_PROMPTS[list(PRESET_PROMPTS.keys())[0]].keys()),\n    description='',\n    layout=widgets.Layout(width='100%')\n)\n\nprompt_input = widgets. Textarea(\n    value=list(PRESET_PROMPTS[list(PRESET_PROMPTS.keys())[0]].values())[0],\n    placeholder='Describe your garment in detail...',\n    layout=widgets.Layout(width='100%', height='80px')\n)\n\nrandom_prompt_btn = widgets.Button(\n    description='üé≤ Surprise Me! ', \n    button_style='warning',\n    layout=widgets.Layout(width='150px')\n)\n\n# --- Settings ---\nsteps_slider = widgets. IntSlider(\n    value=20, min=10, max=40, step=5,\n    description='',\n    layout=widgets.Layout(width='100%'),\n    style={'handle_color': '#667eea'}\n)\n\nguidance_slider = widgets.FloatSlider(\n    value=7.5, min=1.0, max=12.0, step=0.5,\n    description='',\n    layout=widgets. Layout(width='100%'),\n    style={'handle_color':  '#667eea'}\n)\n\nseed_input = widgets.IntText(\n    value=-1,\n    layout=widgets.Layout(width='120px')\n)\n\n# --- Generation Mode ---\nmode_toggle = widgets. ToggleButtons(\n    options=['üé® Single', 'üì¶ Batch (Category)', 'üéØ Batch (Custom)'],\n    value='üé® Single',\n    layout=widgets.Layout(width='100%')\n)\n\n# Batch custom prompts input\nbatch_prompts_input = widgets.Textarea(\n    value=\"A black leather jacket\\nA white silk blouse\\nA red evening dress\\nA navy blazer\",\n    placeholder='Enter one prompt per line (max 6)...',\n    layout=widgets.Layout(width='100%', height='120px')\n)\n\n# --- Action Buttons ---\ngenerate_btn = widgets. Button(\n    description='üöÄ Generate',\n    button_style='success',\n    layout=widgets.Layout(width='200px', height='50px')\n)\n\nclear_btn = widgets.Button(\n    description='üóëÔ∏è Clear Results',\n    button_style='danger',\n    layout=widgets. Layout(width='150px', height='40px')\n)\n\n# --- Output Areas ---\nstatus_output = widgets.Output(layout=widgets.Layout(width='100%', min_height='30px'))\nresults_output = widgets.Output(layout=widgets. Layout(width='100%'))\n\n# ==================== EVENT HANDLERS ====================\n\ndef update_image_preview(*args):\n    \"\"\"Update the image preview.\"\"\"\n    with image_preview: \n        clear_output(wait=True)\n        sample_name = sample_dropdown. value\n        \n        # Use ALL_SAMPLE_PATHS to get the actual path\n        if sample_name and sample_name in ALL_SAMPLE_PATHS:\n            try:\n                img = Image.open(ALL_SAMPLE_PATHS[sample_name]).convert(\"RGB\")\n                \n                fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n                ax.imshow(img)\n                ax.axis('off')\n                ax.set_title(sample_name, fontsize=10, fontweight='bold')\n                plt.tight_layout()\n                plt.show()\n                plt.close()\n            except Exception as e:\n                print(f\"Error loading image: {e}\")\n        else:\n            print(\"No image selected\")\n\ndef on_prev_image(b):\n    \"\"\"Navigate to previous image.\"\"\"\n    options = list(sample_dropdown.options)\n    if not options:\n        return\n    current_idx = options. index(sample_dropdown.value) if sample_dropdown. value in options else 0\n    new_idx = (current_idx - 1) % len(options)\n    sample_dropdown.value = options[new_idx]\n\ndef on_next_image(b):\n    \"\"\"Navigate to next image.\"\"\"\n    options = list(sample_dropdown.options)\n    if not options: \n        return\n    current_idx = options.index(sample_dropdown.value) if sample_dropdown.value in options else 0\n    new_idx = (current_idx + 1) % len(options)\n    sample_dropdown.value = options[new_idx]\n\ndef on_random_image(b):\n    \"\"\"Select random image from current set.\"\"\"\n    options = list(sample_dropdown.options)\n    if options:\n        sample_dropdown.value = random.choice(options)\n\ndef on_shuffle_samples(b):\n    \"\"\"Shuffle to get a completely new random set of samples.\"\"\"\n    global current_samples\n    \n    # Get new random samples from ALL available images\n    new_samples = get_random_samples(10)\n    current_samples = new_samples\n    \n    # Update dropdown options\n    sample_dropdown. options = new_samples\n    sample_dropdown.value = new_samples[0]\n    \n    # Update counter\n    image_counter.value = f'<small style=\"color:#6c757d\">Showing 10 of {len(ALL_SAMPLE_KEYS)} images (shuffled! )</small>'\n    \n    with status_output:\n        clear_output(wait=True)\n        display(HTML('<p style=\"color:#28a745; font-weight: bold;\">üîÄ Loaded new random set of 10 reference images! </p>'))\n\ndef update_prompt_options(change):\n    \"\"\"Update prompt dropdown when category changes.\"\"\"\n    category = change['new']\n    prompts = list(PRESET_PROMPTS[category].keys())\n    prompt_dropdown.options = prompts\n    prompt_dropdown.value = prompts[0]\n    prompt_input.value = PRESET_PROMPTS[category][prompts[0]]\n\ndef update_prompt_text(change):\n    \"\"\"Update prompt text when preset changes.\"\"\"\n    category = category_dropdown.value\n    prompt_name = change['new']\n    if prompt_name in PRESET_PROMPTS[category]:\n        prompt_input.value = PRESET_PROMPTS[category][prompt_name]\n\ndef on_random_prompt(b):\n    \"\"\"Select random category and prompt.\"\"\"\n    category = random.choice(list(PRESET_PROMPTS.keys()))\n    category_dropdown.value = category\n    prompt_name = random.choice(list(PRESET_PROMPTS[category].keys()))\n    prompt_dropdown.value = prompt_name\n\ndef on_clear_results(b):\n    \"\"\"Clear the results area.\"\"\"\n    with results_output: \n        clear_output()\n    with status_output: \n        clear_output()\n        print(\"üßπ Results cleared!\")\n\ndef show_status(message, status_type=\"loading\"):\n    \"\"\"Display status message with styling.\"\"\"\n    with status_output: \n        clear_output(wait=True)\n        if status_type == \"loading\":\n            display(HTML(f'<p class=\"status-loading\">‚è≥ {message}</p>'))\n        elif status_type == \"success\":\n            display(HTML(f'<p class=\"status-success\">‚úÖ {message}</p>'))\n        elif status_type == \"error\":\n            display(HTML(f'<p class=\"status-error\">‚ùå {message}</p>'))\n\ndef display_single_result(reference, edges, generated, prompt, seed):\n    \"\"\"Display single generation result beautifully.\"\"\"\n    fig = plt.figure(figsize=(14, 5))\n    gs = gridspec.GridSpec(1, 4, width_ratios=[1, 1, 0.1, 1.2])\n    \n    # Reference\n    ax1 = fig.add_subplot(gs[0])\n    ax1.imshow(reference)\n    ax1.set_title('üì∑ Reference', fontsize=12, fontweight='bold', color='#495057')\n    ax1.axis('off')\n    \n    # Edge Map\n    ax2 = fig.add_subplot(gs[1])\n    ax2.imshow(edges)\n    ax2.set_title('üî≤ Structure', fontsize=12, fontweight='bold', color='#495057')\n    ax2.axis('off')\n    \n    # Arrow\n    ax_arrow = fig.add_subplot(gs[2])\n    ax_arrow.annotate('', xy=(0.9, 0.5), xytext=(0.1, 0.5),\n                      arrowprops=dict(arrowstyle='->', color='#667eea', lw=3))\n    ax_arrow.axis('off')\n    \n    # Generated\n    ax3 = fig.add_subplot(gs[3])\n    ax3.imshow(generated)\n    ax3.set_title('‚ú® Generated', fontsize=12, fontweight='bold', color='#28a745')\n    ax3.axis('off')\n    \n    # Add prompt as subtitle\n    display_prompt = f'\"{prompt[: 80]}...\"' if len(prompt) > 80 else f'\"{prompt}\"'\n    fig.suptitle(display_prompt, fontsize=10, style='italic', color='#6c757d', y=0.02)\n    \n    plt.tight_layout()\n    \n    filename = f\"generated_seed{seed}.png\"\n    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white', edgecolor='none')\n    plt.show()\n    plt.close()\n    \n    display(HTML(f'''\n        <div style=\"background:#d4edda; padding:10px; border-radius:5px; margin-top:10px; text-align:center;\">\n            <b>üíæ Saved: </b> {filename} | <b>üé≤ Seed:</b> {seed}\n        </div>\n    '''))\n\ndef display_batch_results(reference, edges, results, title_text=\"Batch Generation\"):\n    \"\"\"Display batch generation results in a beautiful grid.\"\"\"\n    n = len(results)\n    \n    if n == 0:\n        print(\"No results to display!\")\n        return\n    \n    # Calculate layout\n    cols = min(3, n)\n    rows = (n + cols - 1) // cols\n    \n    fig = plt.figure(figsize=(16, 3 + rows * 4))\n    \n    # Top row: Reference and Edge\n    gs_top = gridspec.GridSpec(1, 2, figure=fig, left=0.15, right=0.85, top=0.95, bottom=0.68)\n    \n    ax_ref = fig. add_subplot(gs_top[0])\n    ax_ref.imshow(reference)\n    ax_ref.set_title('üì∑ Reference Image', fontsize=12, fontweight='bold')\n    ax_ref.axis('off')\n    \n    ax_edge = fig. add_subplot(gs_top[1])\n    ax_edge.imshow(edges)\n    ax_edge.set_title('üî≤ Extracted Structure', fontsize=12, fontweight='bold')\n    ax_edge.axis('off')\n    \n    # Generated images grid\n    gs_bottom = gridspec.GridSpec(rows, cols, figure=fig, left=0.05, right=0.95, \n                                   top=0.62, bottom=0.02, hspace=0.3, wspace=0.1)\n    \n    for i, (img, prompt) in enumerate(results):\n        row = i // cols\n        col = i % cols\n        ax = fig.add_subplot(gs_bottom[row, col])\n        ax.imshow(img)\n        \n        short_prompt = prompt[: 30] + \"...\" if len(prompt) > 30 else prompt\n        ax.set_title(short_prompt, fontsize=9, fontweight='bold', color='#495057', pad=5)\n        ax.axis('off')\n        \n        for spine in ax.spines.values():\n            spine.set_visible(True)\n            spine.set_color('#667eea')\n            spine.set_linewidth(2)\n    \n    plt. suptitle(title_text, fontsize=14, fontweight='bold', y=0.98)\n    \n    filename = \"batch_generation.png\"\n    plt.savefig(filename, dpi=150, bbox_inches='tight', facecolor='white', edgecolor='none')\n    plt.show()\n    plt.close()\n    \n    display(HTML(f'''\n        <div style=\"background:#d4edda; padding:10px; border-radius:5px; margin-top: 10px; text-align:center;\">\n            <b>üíæ Saved:</b> {filename} | <b>üéØ Generated:</b> {n} images\n        </div>\n    '''))\n\ndef on_generate(b):\n    \"\"\"Handle generation based on selected mode.\"\"\"\n    mode = mode_toggle.value\n    \n    # Get reference image - USE ALL_SAMPLE_PATHS\n    sample_name = sample_dropdown.value\n    if sample_name not in ALL_SAMPLE_PATHS: \n        show_status(\"Please select a reference image!\", \"error\")\n        return\n    \n    ref_image = Image.open(ALL_SAMPLE_PATHS[sample_name]).convert(\"RGB\")\n    ref_image = ref_image.resize((256, 256), Image.LANCZOS)\n    edge_image = get_canny_edge(ref_image)\n    \n    steps = steps_slider. value\n    guidance = guidance_slider. value\n    seed = seed_input. value\n    \n    with results_output:\n        clear_output(wait=True)\n        \n        if mode == 'üé® Single':\n            prompt = prompt_input. value. strip()\n            if not prompt:\n                show_status(\"Please enter a prompt!\", \"error\")\n                return\n            \n            show_status(f\"Generating:  {prompt[:50]}...\")\n            \n            try:\n                if seed == -1:\n                    seed = random. randint(0, 2147483647)\n                generator = torch.Generator(device=\"cuda\").manual_seed(int(seed))\n                \n                with torch.inference_mode():\n                    output = pipe(\n                        prompt=prompt,\n                        image=edge_image,\n                        num_inference_steps=steps,\n                        guidance_scale=guidance,\n                        generator=generator\n                    )\n                \n                display_single_result(ref_image, edge_image, output. images[0], prompt, seed)\n                show_status(f\"Generation complete!  Seed: {seed}\", \"success\")\n                \n            except Exception as e:\n                show_status(f\"Error:  {str(e)}\", \"error\")\n        \n        elif mode == 'üì¶ Batch (Category)':\n            category = category_dropdown.value\n            prompts = list(PRESET_PROMPTS[category].items())[:6]\n            \n            show_status(f\"Generating {len(prompts)} images from '{category}'...\")\n            \n            try:\n                results = []\n                for i, (name, prompt) in enumerate(prompts):\n                    show_status(f\"Generating {i+1}/{len(prompts)}: {name}...\")\n                    \n                    gen_seed = seed + i if seed != -1 else random.randint(0, 2147483647)\n                    generator = torch.Generator(device=\"cuda\").manual_seed(int(gen_seed))\n                    \n                    with torch.inference_mode():\n                        output = pipe(\n                            prompt=prompt,\n                            image=edge_image,\n                            num_inference_steps=steps,\n                            guidance_scale=guidance,\n                            generator=generator\n                        )\n                    \n                    results.append((output.images[0], name))\n                \n                display_batch_results(ref_image, edge_image, results, f\"Batch:  {category}\")\n                show_status(f\"Batch complete!  Generated {len(results)} images.\", \"success\")\n                \n            except Exception as e:\n                show_status(f\"Error: {str(e)}\", \"error\")\n        \n        elif mode == 'üéØ Batch (Custom)':\n            prompts = [p. strip() for p in batch_prompts_input.value.strip().split('\\n') if p.strip()]\n            prompts = prompts[:6]\n            \n            if not prompts: \n                show_status(\"Please enter at least one prompt!\", \"error\")\n                return\n            \n            show_status(f\"Generating {len(prompts)} custom images...\")\n            \n            try:\n                results = []\n                for i, prompt in enumerate(prompts):\n                    show_status(f\"Generating {i+1}/{len(prompts)}: {prompt[:30]}...\")\n                    \n                    gen_seed = seed + i if seed != -1 else random.randint(0, 2147483647)\n                    generator = torch.Generator(device=\"cuda\").manual_seed(int(gen_seed))\n                    \n                    with torch.inference_mode():\n                        output = pipe(\n                            prompt=prompt,\n                            image=edge_image,\n                            num_inference_steps=steps,\n                            guidance_scale=guidance,\n                            generator=generator\n                        )\n                    \n                    results.append((output.images[0], prompt))\n                \n                display_batch_results(ref_image, edge_image, results, \"Batch: Custom Prompts\")\n                show_status(f\"Batch complete!  Generated {len(results)} images.\", \"success\")\n                \n            except Exception as e: \n                show_status(f\"Error:  {str(e)}\", \"error\")\n\n# ==================== CONNECT EVENTS ====================\n\nsample_dropdown.observe(update_image_preview, names='value')\ncategory_dropdown.observe(update_prompt_options, names='value')\nprompt_dropdown.observe(update_prompt_text, names='value')\n\nprev_btn.on_click(on_prev_image)\nnext_btn.on_click(on_next_image)\nrandom_img_btn. on_click(on_random_image)\nshuffle_btn.on_click(on_shuffle_samples)\nrandom_prompt_btn. on_click(on_random_prompt)\ngenerate_btn.on_click(on_generate)\nclear_btn.on_click(on_clear_results)\n\n# Initialize preview\nupdate_image_preview()\n\n# ==================== LAYOUT ====================\n\n# Image Selection Section\nimage_section = widgets.VBox([\n    widgets.HTML('<div class=\"section-title\">üì∑ Reference Image</div>'),\n    sample_dropdown,\n    widgets.HBox([prev_btn, random_img_btn, next_btn], \n                 layout=widgets.Layout(justify_content='center', margin='5px 0')),\n    widgets.HBox([shuffle_btn], \n                 layout=widgets.Layout(justify_content='center', margin='5px 0')),\n    image_counter,\n    image_preview,\n], layout=widgets.Layout(width='300px', padding='15px', \n                          border='1px solid #dee2e6', border_radius='10px',\n                          background_color='#f8f9fa'))\n\n# Prompt Section\nprompt_section = widgets.VBox([\n    widgets.HTML('<div class=\"section-title\">‚úèÔ∏è Prompt</div>'),\n    widgets.HTML('<small style=\"color:#6c757d\">Category: </small>'),\n    category_dropdown,\n    widgets.HTML('<small style=\"color:#6c757d\">Preset:</small>'),\n    widgets.HBox([prompt_dropdown, random_prompt_btn]),\n    widgets.HTML('<small style=\"color:#6c757d\">Or write your own: </small>'),\n    prompt_input,\n], layout=widgets.Layout(width='400px', padding='15px',\n                          border='1px solid #dee2e6', border_radius='10px',\n                          background_color='#f8f9fa'))\n\n# Settings Section\nsettings_section = widgets.VBox([\n    widgets. HTML('<div class=\"section-title\">‚öôÔ∏è Settings</div>'),\n    widgets.HTML('<small style=\"color:#6c757d\">Quality Steps (10-40):</small>'),\n    steps_slider,\n    widgets. HTML('<small style=\"color:#6c757d\">Prompt Strength (1-12):</small>'),\n    guidance_slider,\n    widgets. HBox([\n        widgets.HTML('<small style=\"color:#6c757d; margin-right: 10px\">Seed:</small>'),\n        seed_input,\n        widgets.HTML('<small style=\"color:#999; margin-left:10px\">(-1 = random)</small>')\n    ]),\n], layout=widgets.Layout(width='280px', padding='15px',\n                          border='1px solid #dee2e6', border_radius='10px',\n                          background_color='#f8f9fa'))\n\n# Mode & Batch Section\nmode_section = widgets.VBox([\n    widgets.HTML('<div class=\"section-title\">üé¨ Generation Mode</div>'),\n    mode_toggle,\n    widgets.HTML('<div style=\"margin-top:10px\"><small style=\"color:#6c757d\">Custom batch prompts (one per line, max 6):</small></div>'),\n    batch_prompts_input,\n], layout=widgets.Layout(padding='15px', margin='10px 0',\n                          border='1px solid #dee2e6', border_radius='10px',\n                          background_color='#f8f9fa'))\n\n# Action Section\naction_section = widgets.VBox([\n    widgets.HBox([generate_btn, clear_btn], \n                 layout=widgets.Layout(justify_content='center', gap='20px')),\n    status_output,\n], layout=widgets.Layout(padding='15px', margin='10px 0'))\n\n# Tips\ntips_section = widgets.HTML(\"\"\"\n<div class=\"tip-box\">\n    <b>üí° Pro Tips:</b><br>\n    ‚Ä¢ <b>üîÄ New Set:</b> Get a completely new random set of reference images from all available samples<br>\n    ‚Ä¢ <b>üé≤ Random:</b> Pick a random image from the current set of 10<br>\n    ‚Ä¢ <b>Single Mode:</b> Generate one image with full control<br>\n    ‚Ä¢ <b>Batch (Category):</b> Generate all presets from selected category<br>\n    ‚Ä¢ <b>Batch (Custom):</b> Enter your own prompts, one per line (max 6)\n</div>\n\"\"\")\n\n# Results Section Header\nresults_header = widgets.HTML('<div class=\"results-title\">üñºÔ∏è Generated Results</div>')\n\n# ==================== DISPLAY ====================\n\n# Main Layout\ntop_row = widgets.HBox([image_section, prompt_section, settings_section],\n                       layout=widgets.Layout(gap='15px', justify_content='center'))\n\ndisplay(title)\ndisplay(top_row)\ndisplay(mode_section)\ndisplay(action_section)\ndisplay(tips_section)\ndisplay(results_header)\ndisplay(results_output)\n\nprint(\"‚úÖ Enhanced UI loaded!  Select an image, choose a prompt, and click Generate!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-14T12:21:53.218579Z","iopub.execute_input":"2025-12-14T12:21:53.218872Z","iopub.status.idle":"2025-12-14T12:21:53.436040Z","shell.execute_reply.started":"2025-12-14T12:21:53.218852Z","shell.execute_reply":"2025-12-14T12:21:53.435255Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n<style>\n.widget-label { font-weight: bold ! important; }\n. generator-title { \n    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n    color: white;\n    padding: 20px;\n    border-radius: 10px;\n    margin-bottom: 20px;\n    text-align: center;\n}\n.generator-title h1 { margin: 0; font-size: 28px; }\n. generator-title p { margin: 10px 0 0 0; opacity: 0.9; }\n. section-title { \n    color: #495057; \n    font-size: 16px; \n    font-weight: bold; \n    margin-bottom: 10px;\n    border-bottom: 2px solid #667eea;\n    padding-bottom: 5px;\n}\n. tip-box {\n    background:  linear-gradient(135deg, #f5f7fa 0%, #e4e8eb 100%);\n    border-left: 4px solid #667eea;\n    padding:  15px;\n    margin: 15px 0;\n    border-radius:  0 8px 8px 0;\n}\n.results-title {\n    background: #28a745;\n    color: white;\n    padding: 10px 20px;\n    border-radius: 8px;\n    display: inline-block;\n    margin:  20px 0 10px 0;\n}\n. status-success { color: #28a745; font-weight: bold; }\n.status-error { color: #dc3545; font-weight: bold; }\n.status-loading { color: #ffc107; font-weight: bold; }\n</style>\n"},"metadata":{}},{"name":"stdout","text":"üìÅ Total available images: 100\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"HTML(value='\\n<div class=\"generator-title\">\\n    <h1>üëó Fashion Image Generator</h1>\\n    <p>Transform referenc‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c86c23f1e60644a8a3328fe0652acd2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HBox(children=(VBox(children=(HTML(value='<div class=\"section-title\">üì∑ Reference Image</div>'), Dropdown(layou‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907a54d3d8d748c7a6078fe80a65f610"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<div class=\"section-title\">üé¨ Generation Mode</div>'), ToggleButtons(layout=Layout(w‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b19be7c39cb14bb6b60739a9ad29534f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(HBox(children=(Button(button_style='success', description='üöÄ Generate', layout=Layout(height='5‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0019724629c047d6bfa881168c894bab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HTML(value='\\n<div class=\"tip-box\">\\n    <b>üí° Pro Tips:</b><br>\\n    ‚Ä¢ <b>üîÄ New Set:</b> Get a completely new ‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d35cbff0524c34b0e6ec7303f264c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"HTML(value='<div class=\"results-title\">üñºÔ∏è Generated Results</div>')","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d7e6892193a471abd97ca2a03cfef57"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output(layout=Layout(width='100%'))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ba7dca174854b8fae85dfbc9a20a7d9"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Enhanced UI loaded!  Select an image, choose a prompt, and click Generate!\n","output_type":"stream"}],"execution_count":22}]}