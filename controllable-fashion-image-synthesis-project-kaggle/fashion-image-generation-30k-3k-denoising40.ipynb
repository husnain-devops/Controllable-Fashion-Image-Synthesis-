{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5787457,"sourceType":"datasetVersion","datasetId":3324119}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# CELL 1: Data Preparation\nimport os\nimport h5py\nimport csv\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nimport json\nimport shutil\n\n# --- Configuration ---\nFASHIONGEN_PATH = '/kaggle/input/fashiongen-validation/fashiongen_256_256_train.h5'\nWORKING_DIR = \"/kaggle/working\"\nTRAIN_ROOT = os.path.join(WORKING_DIR, \"fashion_train\")\nTRAIN_IMAGES_DIR = os.path.join(TRAIN_ROOT, \"images\")\nEVAL_ROOT = os.path.join(WORKING_DIR, \"eval_data\")\nEVAL_GT_DIR = os.path.join(EVAL_ROOT, \"gt\")\n\n# Create directories\nif os.path.exists(TRAIN_ROOT): shutil.rmtree(TRAIN_ROOT)\nif os.path.exists(EVAL_ROOT): shutil.rmtree(EVAL_ROOT)\nos.makedirs(TRAIN_IMAGES_DIR, exist_ok=True)\nos.makedirs(EVAL_GT_DIR, exist_ok=True)\n\n# --- Load Data ---\nprint(\"üìÇ Opening dataset...\")\nh5_file = h5py.File(FASHIONGEN_PATH, 'r')\nnum_total = len(h5_file['input_image'])\n\n# Define Split\nTRAIN_SIZE = 30000\nEVAL_SIZE = 3000\ntrain_indices = range(0, TRAIN_SIZE)\neval_indices = range(TRAIN_SIZE, TRAIN_SIZE + EVAL_SIZE)\n\n# --- 1. Export Training Data ---\ntrain_metadata = []\nprint(f\"üöÄ Exporting {TRAIN_SIZE} Training samples...\")\nfor idx in tqdm(train_indices):\n    img = Image.fromarray(h5_file['input_image'][idx])\n    file_name = f\"{idx:06d}.jpg\"\n    img.save(os.path.join(TRAIN_IMAGES_DIR, file_name), quality=95)\n    \n    desc = h5_file['input_description'][idx]\n    if isinstance(desc, bytes): desc = desc.decode('utf-8', errors='ignore')\n    prompt = str(desc).split(',')[0]\n    \n    # Path relative to TRAIN_ROOT\n    train_metadata.append({\"file_name\": f\"images/{file_name}\", \"text\": prompt})\n\nwith open(os.path.join(TRAIN_ROOT, \"metadata.csv\"), 'w', newline='', encoding='utf-8') as f:\n    writer = csv.DictWriter(f, fieldnames=[\"file_name\", \"text\"])\n    writer.writeheader()\n    writer.writerows(train_metadata)\n\n# --- 2. Export Evaluation Data ---\neval_configs = []\nprint(f\"üöÄ Exporting {EVAL_SIZE} Evaluation samples...\")\nfor idx in tqdm(eval_indices):\n    img = Image.fromarray(h5_file['input_image'][idx])\n    img.save(os.path.join(EVAL_GT_DIR, f\"{idx:06d}.png\"))\n    \n    desc = h5_file['input_description'][idx]\n    if isinstance(desc, bytes): desc = desc.decode('utf-8', errors='ignore')\n    prompt = str(desc).split(',')[0]\n    \n    eval_configs.append({\"idx\": idx, \"prompt\": prompt})\n\nwith open(os.path.join(EVAL_ROOT, \"eval_configs.json\"), 'w') as f:\n    json.dump(eval_configs, f)\n\nprint(\"‚úÖ Data Prep Complete.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:44:43.507675Z","iopub.execute_input":"2025-12-13T22:44:43.508469Z","iopub.status.idle":"2025-12-13T22:44:43.961826Z","shell.execute_reply.started":"2025-12-13T22:44:43.508433Z","shell.execute_reply":"2025-12-13T22:44:43.961127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2: Clean Room Installation (Training Stack ONLY)\n\nimport os\n\n# 1. Create a constraints file. \n# This tells pip: \"Under no circumstances allow numpy to be >= 2.0\"\nwith open(\"constraints.txt\", \"w\") as f:\n    f.write(\"numpy<2.0\\n\")\n    f.write(\"huggingface_hub<0.25.0\\n\")\n\n# 2. Uninstall potential conflict creators\nprint(\"üßπ Purging TensorFlow and existing NumPy...\")\nos.system(\"pip uninstall -y tensorflow tensorflow-cpu tensorflow-gpu protobuf numpy\")\n\n# 3. Install Training Stack with constraints\nprint(\"üì¶ Installing Training Stack...\")\n# We use a single command to ensure dependency resolution sees all packages at once\ncmd = (\n    \"pip install \"\n    \"numpy==1.26.4 \"  # Explicitly force safe numpy\n    \"diffusers==0.26.3 \"\n    \"transformers==4.38.2 \"\n    \"accelerate==0.27.2 \"\n    \"peft==0.9.0 \"\n    \"bitsandbytes==0.41.3 \"\n    \"ftfy \"\n    \"tensorboard \"\n    \"-c constraints.txt\" # Apply strict constraints\n)\nos.system(cmd)\n\n# 4. Download Training Script\nprint(\"üìú Downloading Script...\")\nos.system(\"wget -q https://raw.githubusercontent.com/huggingface/diffusers/v0.26.3/examples/text_to_image/train_text_to_image_lora.py\")\n\nimport numpy\nprint(f\"‚úÖ Install Complete. NumPy Version: {numpy.__version__}\")\nif numpy.__version__.startswith(\"2\"):\n    raise RuntimeError(\"STOP! NumPy 2.0 is still present. Restart the kernel.\")\n\n# CELL 2 (PATCH): Uninstall Broken bitsandbytes\nimport os\n\nprint(\"üöë Removing broken bitsandbytes library...\")\n# We uninstall it so peft stops trying to import it and crashing.\nos.system(\"pip uninstall -y bitsandbytes\")\n\nprint(\"‚úÖ Broken library removed. Falling back to standard precision.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:44:47.912045Z","iopub.execute_input":"2025-12-13T22:44:47.912695Z","iopub.status.idle":"2025-12-13T22:46:40.544728Z","shell.execute_reply.started":"2025-12-13T22:44:47.912643Z","shell.execute_reply":"2025-12-13T22:46:40.544006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 3: Run Training (FP32 Safe Mode - No BNB)\n\nOUTPUT_DIR = \"/kaggle/working/fashion_lora_output\"\n\n!accelerate launch train_text_to_image_lora.py \\\n  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\" \\\n  --train_data_dir=\"/kaggle/working/fashion_train\" \\\n  --caption_column=\"text\" \\\n  --resolution=256 \\\n  --random_flip \\\n  --train_batch_size=2 \\\n  --gradient_accumulation_steps=2 \\\n  --max_train_steps=5000 \\\n  --learning_rate=1e-04 \\\n  --max_grad_norm=1 \\\n  --lr_scheduler=\"cosine\" \\\n  --lr_warmup_steps=500 \\\n  --output_dir={OUTPUT_DIR} \\\n  --checkpointing_steps=1000 \\\n  --seed=42 \\\n  --report_to=\"tensorboard\"\n\nprint(\"‚úÖ Training Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:46:49.983768Z","iopub.execute_input":"2025-12-13T22:46:49.98404Z","iopub.status.idle":"2025-12-13T22:47:41.740981Z","shell.execute_reply.started":"2025-12-13T22:46:49.98402Z","shell.execute_reply":"2025-12-13T22:47:41.740151Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 3.5: Plot Training Loss from TensorBoard Logs\nimport matplotlib.pyplot as plt\nfrom tensorboard.backend.event_processing.event_accumulator import EventAccumulator\nimport glob\nimport os\n\nprint(\"üìä Extracting training loss...\")\nlog_dir = \"/kaggle/working/fashion_lora_output\"\n\n# Find the events file (it's inside a subfolder usually)\nevent_files = glob.glob(f\"{log_dir}/**/events.out.tfevents.*\", recursive=True)\n\nif event_files:\n    # Load the most recent event file\n    ea = EventAccumulator(event_files[0])\n    ea.Reload()\n    \n    # Check available tags\n    tags = ea.Tags()['scalars']\n    if 'train_loss' in tags:\n        losses = ea.Scalars('train_loss')\n        steps = [x.step for x in losses]\n        vals = [x.value for x in losses]\n        \n        # Plot\n        plt.figure(figsize=(10, 6))\n        plt.plot(steps, vals, label=\"Train Loss\", color='blue', alpha=0.6)\n        \n        # Add a moving average for smoothing\n        if len(vals) > 20:\n            window = 20\n            avg_vals = [sum(vals[i:i+window])/window for i in range(len(vals)-window)]\n            plt.plot(steps[window:], avg_vals, color='red', linewidth=2, label='Moving Avg')\n\n        plt.xlabel(\"Step\")\n        plt.ylabel(\"Loss\")\n        plt.title(\"Training Loss Curve\")\n        plt.grid(True, alpha=0.3)\n        plt.legend()\n        \n        # Save\n        plt.savefig(\"training_loss.png\", dpi=150)\n        print(\"‚úÖ Saved 'training_loss.png'\")\n        plt.show()\n    else:\n        print(\"‚ö†Ô∏è 'train_loss' not found in logs. (Did you run for enough steps?)\")\nelse:\n    print(\"‚ö†Ô∏è No TensorBoard logs found in output directory.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:47:45.755594Z","iopub.execute_input":"2025-12-13T22:47:45.756346Z","iopub.status.idle":"2025-12-13T22:47:46.360796Z","shell.execute_reply.started":"2025-12-13T22:47:45.756317Z","shell.execute_reply":"2025-12-13T22:47:46.360165Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 4: Install Needed Libraries\n\nimport os\n\nprint(\"üì¶ Installing Evaluation Tools...\")\nos.system(\"pip install clean-fid lpips 'scikit-image<0.23.0' opencv-python-headless torch-fidelity 'numpy<2.0'\")\n\nprint(\"‚úÖ Eval Tools Installed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:47:49.93654Z","iopub.execute_input":"2025-12-13T22:47:49.937435Z","iopub.status.idle":"2025-12-13T22:47:58.877585Z","shell.execute_reply.started":"2025-12-13T22:47:49.937409Z","shell.execute_reply":"2025-12-13T22:47:58.876942Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 5: Generate Images for Evaluation\nimport torch\nimport cv2\nimport numpy as np\nimport json\nimport os\nfrom PIL import Image\nfrom diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\nfrom tqdm.auto import tqdm\n\n# --- Config ---\nEVAL_ROOT = \"/kaggle/working/eval_data\"\nGT_DIR = os.path.join(EVAL_ROOT, \"gt\")\nBASE_DIR = os.path.join(EVAL_ROOT, \"baseline\")\nLORA_DIR = os.path.join(EVAL_ROOT, \"lora\")\nos.makedirs(BASE_DIR, exist_ok=True)\nos.makedirs(LORA_DIR, exist_ok=True)\n\nwith open(os.path.join(EVAL_ROOT, \"eval_configs.json\"), 'r') as f:\n    eval_configs = json.load(f)\n\ndef get_canny_edge(pil_img):\n    img = np.array(pil_img)\n    edges = cv2.Canny(img, 100, 200)\n    edges = np.stack([edges]*3, axis=-1)\n    return Image.fromarray(edges)\n\n# --- Load Pipeline ---\nprint(\"‚öôÔ∏è Loading Pipeline...\")\ncontrolnet = ControlNetModel.from_pretrained(\"lllyasviel/control_v11p_sd15_canny\", torch_dtype=torch.float16)\npipe = StableDiffusionControlNetPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16, safety_checker=None\n).to(\"cuda\")\npipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n\n# --- 1. Generate Baseline (ControlNet Only) ---\nprint(\"üöÄ Generating Baseline Images...\")\nfor item in tqdm(eval_configs, desc=\"Baseline\"):\n    idx = item['idx']\n    if os.path.exists(os.path.join(BASE_DIR, f\"{idx:06d}.png\")): continue\n    \n    gt_img = Image.open(os.path.join(GT_DIR, f\"{idx:06d}.png\")).convert(\"RGB\")\n    edge_img = get_canny_edge(gt_img)\n    \n    with torch.inference_mode():\n        gen = pipe(item['prompt'], image=edge_img, num_inference_steps=40, guidance_scale = 10).images[0]\n    gen.save(os.path.join(BASE_DIR, f\"{idx:06d}.png\"))\n\n# --- 2. Generate LoRA (ControlNet + Your Style) ---\nprint(\"üöÄ Generating LoRA Images...\")\npipe.load_lora_weights(\"/kaggle/working/fashion_lora_output\", weight_name=\"pytorch_lora_weights.safetensors\")\n\nfor item in tqdm(eval_configs, desc=\"LoRA\"):\n    idx = item['idx']\n    if os.path.exists(os.path.join(LORA_DIR, f\"{idx:06d}.png\")): continue\n    \n    gt_img = Image.open(os.path.join(GT_DIR, f\"{idx:06d}.png\")).convert(\"RGB\")\n    edge_img = get_canny_edge(gt_img)\n    \n    with torch.inference_mode():\n        gen = pipe(item['prompt'], image=edge_img, num_inference_steps=40, guidance_scale = 10).images[0]\n    gen.save(os.path.join(LORA_DIR, f\"{idx:06d}.png\"))\n\nprint(\"‚úÖ Generation Complete.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:48:12.066917Z","iopub.execute_input":"2025-12-13T22:48:12.067407Z","iopub.status.idle":"2025-12-13T22:48:49.365534Z","shell.execute_reply.started":"2025-12-13T22:48:12.067384Z","shell.execute_reply":"2025-12-13T22:48:49.364774Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 2: Locate Data\nimport os\nimport shutil\n\nif os.path.exists(\"/kaggle/working/eval_data\"):\n    INPUT_ROOT = \"/kaggle/working\"\nelse:\n    # Check standard Kaggle input structure\n    possible_roots = [d for d in os.listdir(\"/kaggle/input\") if os.path.isdir(os.path.join(\"/kaggle/input\", d))]\n    for d in possible_roots:\n        if os.path.exists(os.path.join(\"/kaggle/input\", d, \"eval_data\")):\n            INPUT_ROOT = os.path.join(\"/kaggle/input\", d)\n            print(f\"üîé Auto-detected input root: {INPUT_ROOT}\")\n            break\n\nEVAL_DATA_PATH = os.path.join(INPUT_ROOT, \"eval_data\")\nGT_DIR = os.path.join(EVAL_DATA_PATH, \"gt\")\nBASE_DIR = os.path.join(EVAL_DATA_PATH, \"baseline\")\nLORA_DIR = os.path.join(EVAL_DATA_PATH, \"lora\")\nCONFIG_PATH = os.path.join(EVAL_DATA_PATH, \"eval_configs.json\")\n\n# Verify\nif os.path.exists(GT_DIR) and os.path.exists(LORA_DIR):\n    print(f\"‚úÖ Found Data at: {EVAL_DATA_PATH}\")\n    print(f\"   - GT Images: {len(os.listdir(GT_DIR))}\")\n    print(f\"   - Baseline Images: {len(os.listdir(BASE_DIR))}\")\n    print(f\"   - LoRA Images: {len(os.listdir(LORA_DIR))}\")\nelse:\n    raise FileNotFoundError(f\"‚ùå Could not find 'eval_data' in {INPUT_ROOT}. Please check the path.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:48:52.946335Z","iopub.execute_input":"2025-12-13T22:48:52.947033Z","iopub.status.idle":"2025-12-13T22:48:52.954316Z","shell.execute_reply.started":"2025-12-13T22:48:52.947009Z","shell.execute_reply":"2025-12-13T22:48:52.953702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL: Visual Comparison WITH Text Prompts\nimport matplotlib.pyplot as plt\nimport textwrap\nimport random\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport os\nimport json\n\n# Load Configs\nwith open(CONFIG_PATH, 'r') as f:\n    configs = json.load(f)\n\ndef show_results_with_text(n=5, save_path=\"qualitative_comparison.png\"):\n    # Pick random samples\n    samples = random.sample(configs, n)\n    \n    # Create a figure: N rows, 4 cols\n    # We make the figure taller (height=5 per row) to make room for text\n    fig, axes = plt.subplots(n, 4, figsize=(16, 5*n))\n    \n    # Adjust spacing to fit text\n    plt.subplots_adjust(hspace=0.6, wspace=0.1)\n\n    for i, item in enumerate(samples):\n        idx = item['idx']\n        raw_prompt = item['prompt']\n        \n        # 1. Clean and Wrap Text\n        # Wrap text to 80 chars per line so it fits neatly\n        wrapper = textwrap.TextWrapper(width=70)\n        wrapped_prompt = \"\\n\".join(wrapper.wrap(f\"PROMPT: {raw_prompt}\"))\n        \n        # 2. Load Images\n        gt = Image.open(os.path.join(GT_DIR, f\"{idx:06d}.png\"))\n        base = Image.open(os.path.join(BASE_DIR, f\"{idx:06d}.png\"))\n        lora = Image.open(os.path.join(LORA_DIR, f\"{idx:06d}.png\"))\n        \n        # Create Edge Map\n        edge = cv2.Canny(np.array(gt), 100, 200)\n        edge = Image.fromarray(np.stack([edge]*3, axis=-1))\n        \n        # 3. Add Text Label above the first image of the row\n        # (x=0, y=1.2 in axis coordinates puts it above the plot)\n        axes[i, 0].text(\n            0, 1.15, \n            wrapped_prompt, \n            transform=axes[i, 0].transAxes, \n            fontsize=12, \n            fontweight='bold', \n            verticalalignment='bottom',\n            bbox=dict(facecolor='white', alpha=0.8, edgecolor='none')\n        )\n\n        # 4. Plot Images\n        # Column 1: Ground Truth\n        axes[i,0].imshow(gt)\n        axes[i,0].set_title(\"Ground Truth (Target)\", fontsize=10)\n        axes[i,0].axis('off')\n        \n        # Column 2: Edge Map\n        axes[i,1].imshow(edge)\n        axes[i,1].set_title(\"Input Structure (Edge)\", fontsize=10)\n        axes[i,1].axis('off')\n        \n        # Column 3: Baseline\n        axes[i,2].imshow(base)\n        axes[i,2].set_title(\"Baseline (Structure Only)\", fontsize=10)\n        axes[i,2].axis('off')\n        \n        # Column 4: Ours\n        axes[i,3].imshow(lora)\n        axes[i,3].set_title(\"Ours (Structure + Style)\", fontsize=10)\n        axes[i,3].axis('off')\n\n    # Save\n    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n    plt.show()\n    print(f\"‚úÖ Saved neat comparison to: {save_path}\")\n\n# Run it\nshow_results_with_text(n=5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:49:13.861758Z","iopub.execute_input":"2025-12-13T22:49:13.862433Z","iopub.status.idle":"2025-12-13T22:49:15.501002Z","shell.execute_reply.started":"2025-12-13T22:49:13.86241Z","shell.execute_reply":"2025-12-13T22:49:15.500137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 3: Calculate Metrics (Fixed CLIP Truncation)\nimport torch\nimport numpy as np\nimport os\nimport json\nfrom cleanfid import fid\nimport lpips\nfrom transformers import CLIPProcessor, CLIPModel\nfrom PIL import Image\nfrom tqdm.auto import tqdm\n\n# Load Configs\nwith open(CONFIG_PATH, 'r') as f:\n    eval_configs = json.load(f)\n\nmetrics = {\"baseline\": {}, \"lora\": {}}\n\n# --- 1. FID & KID ---\nprint(\"üìè Calculating FID and KID... (This takes a few minutes)\")\n# Baseline\nmetrics[\"baseline\"][\"fid\"] = fid.compute_fid(GT_DIR, BASE_DIR, mode=\"clean\", num_workers=0)\nmetrics[\"baseline\"][\"kid\"] = fid.compute_kid(GT_DIR, BASE_DIR, mode=\"clean\", num_workers=0)\n# LoRA\nmetrics[\"lora\"][\"fid\"] = fid.compute_fid(GT_DIR, LORA_DIR, mode=\"clean\", num_workers=0)\nmetrics[\"lora\"][\"kid\"] = fid.compute_kid(GT_DIR, LORA_DIR, mode=\"clean\", num_workers=0)\n\n# --- 2. LPIPS ---\nprint(\"üìè Calculating LPIPS...\")\nloss_fn_alex = lpips.LPIPS(net='alex').to(\"cuda\")\n\ndef calc_lpips(dir1, dir2):\n    scores = []\n    files = os.listdir(dir1)\n    for f in tqdm(files, desc=\"LPIPS\", leave=False):\n        if not f.endswith(\".png\"): continue\n        p1 = os.path.join(dir1, f)\n        p2 = os.path.join(dir2, f)\n        if os.path.exists(p2):\n            img1 = lpips.im2tensor(lpips.load_image(p1)).to(\"cuda\")\n            img2 = lpips.im2tensor(lpips.load_image(p2)).to(\"cuda\")\n            with torch.no_grad():\n                scores.append(loss_fn_alex(img1, img2).item())\n    return np.mean(scores)\n\nmetrics[\"baseline\"][\"lpips\"] = calc_lpips(GT_DIR, BASE_DIR)\nmetrics[\"lora\"][\"lpips\"] = calc_lpips(GT_DIR, LORA_DIR)\n\n# --- 3. CLIP Score (FIXED) ---\nprint(\"üìè Calculating CLIP Score...\")\nclip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\nclip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\ndef calc_clip(image_dir, configs):\n    scores = []\n    for item in tqdm(configs, desc=\"CLIP\", leave=False):\n        idx = item['idx']\n        prompt = item['prompt']\n        img_path = os.path.join(image_dir, f\"{idx:06d}.png\")\n        if os.path.exists(img_path):\n            image = Image.open(img_path)\n            # FIX: Added truncation=True and max_length=77\n            inputs = clip_processor(\n                text=[prompt], \n                images=image, \n                return_tensors=\"pt\", \n                padding=True, \n                truncation=True, \n                max_length=77\n            ).to(\"cuda\")\n            \n            with torch.no_grad():\n                outputs = clip_model(**inputs)\n                logits_per_image = outputs.logits_per_image \n                scores.append(logits_per_image.item() / 100.0) \n    return np.mean(scores)\n\nmetrics[\"baseline\"][\"clip\"] = calc_clip(BASE_DIR, eval_configs)\nmetrics[\"lora\"][\"clip\"] = calc_clip(LORA_DIR, eval_configs)\n\n# --- Save ---\nwith open(\"final_metrics_recalculated.json\", \"w\") as f:\n    json.dump(metrics, f, indent=4)\n\nprint(\"\\n\" + \"=\"*40)\nprint(\"üìä RECALCULATED RESULTS\")\nprint(\"=\"*40)\nprint(json.dumps(metrics, indent=4))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:49:50.227798Z","iopub.execute_input":"2025-12-13T22:49:50.228095Z","iopub.status.idle":"2025-12-13T22:50:18.461662Z","shell.execute_reply.started":"2025-12-13T22:49:50.228076Z","shell.execute_reply":"2025-12-13T22:50:18.461051Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CELL 5: Zip Everything\n\noutput_zip = \"final_evaluation_results.zip\"\nprint(f\"üì¶ Zipping to {output_zip}...\")\n\nwith zipfile.ZipFile(output_zip, 'w', zipfile.ZIP_DEFLATED) as zf:\n    # Add the new metrics and plot\n    zf.write(\"final_metrics_recalculated.json\")\n    zf.write(\"qualitative_comparison_with_text.png\")\n    \n    # Try to add the training loss plot if it exists in the input\n    # (It might be in the root of the input folder)\n    loss_plot_path = os.path.join(INPUT_ROOT, \"training_loss.png\")\n    if os.path.exists(loss_plot_path):\n        zf.write(loss_plot_path, \"training_loss.png\")\n    \n    # Add weights if available\n    weights_path = os.path.join(INPUT_ROOT, \"fashion_lora_output\", \"pytorch_lora_weights.safetensors\")\n    if os.path.exists(weights_path):\n        zf.write(weights_path, \"weights/pytorch_lora_weights.safetensors\")\n\nprint(\"‚úÖ DONE! Download 'final_results.zip'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-13T22:50:26.732965Z","iopub.execute_input":"2025-12-13T22:50:26.733725Z","iopub.status.idle":"2025-12-13T22:50:26.761396Z","shell.execute_reply.started":"2025-12-13T22:50:26.733699Z","shell.execute_reply":"2025-12-13T22:50:26.760495Z"}},"outputs":[],"execution_count":null}]}